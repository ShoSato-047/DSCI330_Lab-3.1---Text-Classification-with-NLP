{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShoSato-047/DSCI330_Lab-3.1---Text-Classification-with-NLP/blob/main/DSCI330_lab3_1_text_classification_w_NLP_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = red>**BoW vs. TF-IDF:**\n",
        "- Use BoW when you need a simple word frequency representation.\n",
        "- Use TF-IDF when you want to weigh words based on importance across documents."
      ],
      "metadata": {
        "id": "CjzZU3BxzP0A"
      },
      "id": "CjzZU3BxzP0A"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "25ef8595-3418-4e44-99c4-048330182d3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25ef8595-3418-4e44-99c4-048330182d3f",
        "outputId": "5eaab7da-783b-4be6-9479-a0c2ef63c454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install composable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6W1SFtCHzd79",
        "outputId": "ed471f24-4c24-41f1-a5e1-89ea10980547"
      },
      "id": "6W1SFtCHzd79",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: composable in /usr/local/lib/python3.11/dist-packages (0.5.4)\n",
            "Requirement already satisfied: python-forge<19.0,>=18.6 in /usr/local/lib/python3.11/dist-packages (from composable) (18.6.0)\n",
            "Requirement already satisfied: toolz<0.12.0,>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from composable) (0.11.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f8a8cdf1-1bce-4915-9a45-fcf072ea00b2",
      "metadata": {
        "id": "f8a8cdf1-1bce-4915-9a45-fcf072ea00b2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import composable_records as rec\n",
        "import composable_tuples as tup\n",
        "\n",
        "from composable import pipeable\n",
        "from composable.strict import map, filter\n",
        "from composable_utility import apply, identity, get\n",
        "from composable_object import obj\n",
        "\n",
        "from composable_glob import glob\n",
        "from composable_utility import get, with_open, identity, apply\n",
        "\n",
        "# You may need to install utility.py to use \"with_open\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is the root directory\n",
        "!pwd"
      ],
      "metadata": {
        "id": "ye0mWojl7DwA",
        "outputId": "ff933227-a05a-4be9-8a4d-5b08add67d07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ye0mWojl7DwA",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31f3e62d-8aae-4a16-868f-c8dfc9e9becd",
      "metadata": {
        "id": "31f3e62d-8aae-4a16-868f-c8dfc9e9becd"
      },
      "source": [
        "## Loading the emotions dataset\n",
        "\n",
        "The following data set include a large number of sentences combined with a classification of the emotion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "1793c54d-1179-4111-ab5e-e08e9dbc6c08",
      "metadata": {
        "id": "1793c54d-1179-4111-ab5e-e08e9dbc6c08"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "emotions = load_dataset(\"emotion\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "f97c30ec-9fc3-4771-9bcf-34e712a6dba3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f97c30ec-9fc3-4771-9bcf-34e712a6dba3",
        "outputId": "0437b191-9905-4494-ef84-eff28076c1dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 16000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "emotions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "3f944535-d332-4da7-b624-56d4c049c4fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f944535-d332-4da7-b624-56d4c049c4fb",
        "outputId": "05e667aa-ce02-4e66-a94c-8c80fd9fc28a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'label'],\n",
              "    num_rows: 16000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "(train_ds := emotions >> get('train')) #getting training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "0e040da0-9806-4863-a894-11189d49c7e6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e040da0-9806-4863-a894-11189d49c7e6",
        "outputId": "896de8ad-7608-4e91-cde8-18618c6efe3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['text', 'label']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "train_ds.column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "9785c036-f4f4-4c64-9d8d-b7626d178a42",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9785c036-f4f4-4c64-9d8d-b7626d178a42",
        "outputId": "225fa10f-3c52-44e0-dcbc-6518eec8c1b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': Value(dtype='string', id=None),\n",
              " 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], id=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "train_ds.features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "b25f8a9b-d698-4e2d-8a03-f2308807526e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b25f8a9b-d698-4e2d-8a03-f2308807526e",
        "outputId": "abe6aaab-1d6c-4e12-a367-5c97d0c78a35"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ['i didnt feel humiliated',\n",
              "  'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n",
              "  'im grabbing a minute to post i feel greedy wrong',\n",
              "  'i am ever feeling nostalgic about the fireplace i will know that it is still on the property',\n",
              "  'i am feeling grouchy'],\n",
              " 'label': [0, 0, 3, 2, 3]}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "train_ds[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab241e0e-7536-4ca4-a1bd-e61a0716b827",
      "metadata": {
        "id": "ab241e0e-7536-4ca4-a1bd-e61a0716b827"
      },
      "source": [
        "### Preparing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "11294a45-3901-4432-922c-15ef15f2ba98",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11294a45-3901-4432-922c-15ef15f2ba98",
        "outputId": "f56a0675-aa92-4c87-9576-d564e2bbb3b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i didnt feel humiliated',\n",
              " 'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n",
              " 'im grabbing a minute to post i feel greedy wrong',\n",
              " 'i am ever feeling nostalgic about the fireplace i will know that it is still on the property',\n",
              " 'i am feeling grouchy']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "(documents :=\n",
        " train_ds\n",
        " >> get('text')\n",
        ") >> tup.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "7e7bcad8-0436-4fbf-8c63-c4d364ec89f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e7bcad8-0436-4fbf-8c63-c4d364ec89f9",
        "outputId": "3af1fcd3-48b1-49f2-93c4-94e3fb126db3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 3, 2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "source": [
        "(labels :=\n",
        " train_ds\n",
        " >> get('label')\n",
        ") >> tup.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96f39040-f15f-4ccc-97bb-648c334fecc1",
      "metadata": {
        "id": "96f39040-f15f-4ccc-97bb-648c334fecc1"
      },
      "source": [
        "### Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bag of Words:**\n",
        "In Bag of Words (BoW), the higher the value, the more frequently a word appears in a document.\n",
        "\n",
        "The score is not limited to a range of 0 to 1. The values represent raw word counts, meaning they can be any non-negative integer starting from 0 and increasing based on how often a word appears in a document.\n",
        "\n",
        "- Minimum: 0 (if the word is absent in a document)\n",
        "- Maximum: No fixed upper limit (depends on how frequently a word appears)"
      ],
      "metadata": {
        "id": "PHzM9KlY0fDa"
      },
      "id": "PHzM9KlY0fDa"
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "93641b34-6fa7-4859-9a40-1295a10e951f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93641b34-6fa7-4859-9a40-1295a10e951f",
        "outputId": "bc540494-31f2-40c8-e1a7-d794fb31ff5d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7375"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create the sparse feature set\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Create a training and test (validation) set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "# Train the model\n",
        "classifier = MultinomialNB()\n",
        "\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "(accuracy := accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b6d915b-75d7-4f27-b1f3-bed1d79ebde5",
      "metadata": {
        "id": "8b6d915b-75d7-4f27-b1f3-bed1d79ebde5"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TF-IDF Formula:**\n",
        "\n",
        "**TF-IDF = TF × IDF**\n",
        "\n",
        "Where:\n",
        "\n",
        "- Term Frequency (TF): Measures how often a term appears in a document.\n",
        "- Inverse Document Frequency (IDF): Measures how rare a term is across documents.\n",
        "\n",
        "Note:\n",
        "- Minimum Score: 0 - If a word does not appear in a document, its score is 0.\n",
        "- If *TfidfVectorizer* uses **L2 normalization** (default in scikit-learn), TF-IDF values are scaled to range between 0 and 1 for each document."
      ],
      "metadata": {
        "id": "KZqtM8PqyPwC"
      },
      "id": "KZqtM8PqyPwC"
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "45044ca8-2bcc-47c4-aab6-7babad73569d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45044ca8-2bcc-47c4-aab6-7babad73569d",
        "outputId": "1412ba42-1aeb-4f3e-c7cc-e89d1fb8e521"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6165625"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create the sparse feature set\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Create a training and test (validation) set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "# Train the model\n",
        "classifier = MultinomialNB()\n",
        "\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "(accuracy := accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"red\"> Exercise 3.6.1 </font>\n",
        "\n",
        "The file `train.csv` contains bits of text from three classic authors in the horror genre ([source](https://www.kaggle.com/c/spooky-author-identification/)).\n",
        "\n",
        "**Preprocessing Tasks.**\n",
        "1. Read the raw lines in using `with_open`.\n",
        "2. Split the data into columns and extract the text into one list and the labels into other.\n",
        "3. Clean up the text by making it lower case and removing any punctuation.\n",
        "4. Map the text labels to numbers.\n",
        "\n",
        "**ML tasks.** Test the performance of the naive Bayes classifer on both the Bag of Words and TF-IDF features."
      ],
      "metadata": {
        "id": "NVIzI4rp0sEn"
      },
      "id": "NVIzI4rp0sEn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Question:\n",
        "\n",
        "# Your objective is to accurately identify the author of the sentences in the test set?\n",
        "\n",
        "# What do you mean by labels? Is this a list of specific words for each author?\n",
        "# We had a list of label ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'] in emotions dataset.\n",
        "# Where can I find a list of labels?\n",
        "\n",
        "# Am I supposed to split the text for each author?"
      ],
      "metadata": {
        "id": "Y78tMyqZBdgr"
      },
      "id": "Y78tMyqZBdgr",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "XxRXQXUv1yj_"
      },
      "id": "XxRXQXUv1yj_",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1: Read the raw lines in using with_open**"
      ],
      "metadata": {
        "id": "zXC6IpsyrHSH"
      },
      "id": "zXC6IpsyrHSH"
    },
    {
      "cell_type": "code",
      "source": [
        "# This is my current location (root)\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKBCp8JY_EXw",
        "outputId": "8c43f388-a994-490f-d408-e69129cbea2f"
      },
      "id": "jKBCp8JY_EXw",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(paths :=\n",
        " \"/content/train.csv\"\n",
        " >> glob()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHDxrXX3_C23",
        "outputId": "52968ede-ca2a-4e2e-a402-7ea232b86344"
      },
      "id": "nHDxrXX3_C23",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/train.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(paths[0], encoding =\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "lines[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phppdOLq--p2",
        "outputId": "f9247b48-5d4f-4eef-f76d-ae7ef0f33faf"
      },
      "id": "phppdOLq--p2",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\"id\",\"text\",\"author\"\\n',\n",
              " '\"id26305\",\"This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.\",\"EAP\"\\n',\n",
              " '\"id17569\",\"It never once occurred to me that the fumbling might be a mere mistake.\",\"HPL\"\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2: Split the data into columns and extract the text into one list and the labels into other**"
      ],
      "metadata": {
        "id": "IBPh08bPrTX-"
      },
      "id": "IBPh08bPrTX-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Hint: Replace ',' with '-' in each line\n",
        "lines = [line.replace(', ', '- ') for line in lines]\n",
        "lines[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XubaTeOI_sM0",
        "outputId": "05a23b7c-29cb-4178-8fa4-082b072c238f"
      },
      "id": "XubaTeOI_sM0",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\"id\",\"text\",\"author\"\\n',\n",
              " '\"id26305\",\"This process- however- afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit- and return to the point whence I set out- without being aware of the fact; so perfectly uniform seemed the wall.\",\"EAP\"\\n',\n",
              " '\"id17569\",\"It never once occurred to me that the fumbling might be a mere mistake.\",\"HPL\"\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_lines = [line.split(',') for line in lines]\n",
        "split_lines[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKgfYb8t-gyo",
        "outputId": "469f7faf-0437-4dd9-f47b-453cf311d4e0"
      },
      "id": "tKgfYb8t-gyo",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['\"id\"', '\"text\"', '\"author\"\\n'],\n",
              " ['\"id26305\"',\n",
              "  '\"This process- however- afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit- and return to the point whence I set out- without being aware of the fact; so perfectly uniform seemed the wall.\"',\n",
              "  '\"EAP\"\\n'],\n",
              " ['\"id17569\"',\n",
              "  '\"It never once occurred to me that the fumbling might be a mere mistake.\"',\n",
              "  '\"HPL\"\\n']]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a record\n",
        "records = []\n",
        "for row in split_lines[1:]:  # Skip header\n",
        "    record = {\n",
        "        \"id\": row[0].strip('\"'),\n",
        "        \"text\": row[1].strip('\"'),\n",
        "        \"author\": row[2].strip('\"').strip('\\n')\n",
        "    }\n",
        "    records.append(record)\n",
        "\n",
        "records[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q9waJd6BM0U",
        "outputId": "17ac7bba-695a-4d12-b57b-ef5a7513b89a"
      },
      "id": "9Q9waJd6BM0U",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'id26305',\n",
              "  'text': 'This process- however- afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit- and return to the point whence I set out- without being aware of the fact; so perfectly uniform seemed the wall.',\n",
              "  'author': 'EAP\"'},\n",
              " {'id': 'id17569',\n",
              "  'text': 'It never once occurred to me that the fumbling might be a mere mistake.',\n",
              "  'author': 'HPL\"'},\n",
              " {'id': 'id11008',\n",
              "  'text': 'In his left hand was a gold snuff box- from which- as he capered down the hill- cutting all manner of fantastic steps- he took snuff incessantly with an air of the greatest possible self satisfaction.',\n",
              "  'author': 'EAP\"'}]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# double check if my records work correctly\n",
        "records[0]['id']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "71se1opCOdXe",
        "outputId": "850efd63-e61f-4001-a06e-f1c7f3296fde"
      },
      "id": "71se1opCOdXe",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'id26305'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3: Clean up the text by making it lower case and removing any punctuation.**"
      ],
      "metadata": {
        "id": "DCkQvVvhIv8j"
      },
      "id": "DCkQvVvhIv8j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Comprehensions Solution:**"
      ],
      "metadata": {
        "id": "S5E1KxPygK2i"
      },
      "id": "S5E1KxPygK2i"
    },
    {
      "cell_type": "code",
      "source": [
        "# Making text lowercase\n",
        "text_lower = [record['text'].lower() for record in records]\n",
        "text_lower[:3]"
      ],
      "metadata": {
        "id": "bvfFUIGlIx13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77a56423-61fa-48f0-97c5-19623e739e0d"
      },
      "id": "bvfFUIGlIx13",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this process- however- afforded me no means of ascertaining the dimensions of my dungeon; as i might make its circuit- and return to the point whence i set out- without being aware of the fact; so perfectly uniform seemed the wall.',\n",
              " 'it never once occurred to me that the fumbling might be a mere mistake.',\n",
              " 'in his left hand was a gold snuff box- from which- as he capered down the hill- cutting all manner of fantastic steps- he took snuff incessantly with an air of the greatest possible self satisfaction.']"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaXYLOrRsdoI",
        "outputId": "8ed69886-7dcf-431d-990e-41c0e9b180f7"
      },
      "id": "SaXYLOrRsdoI",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing a list of sentence into words\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text_words = [word_tokenize(record['text']) for record in records]\n",
        "text_words[:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVzap9AQsBtE",
        "outputId": "00445667-2954-450e-aa77-5eca6b8fc337"
      },
      "id": "qVzap9AQsBtE",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['This',\n",
              "  'process-',\n",
              "  'however-',\n",
              "  'afforded',\n",
              "  'me',\n",
              "  'no',\n",
              "  'means',\n",
              "  'of',\n",
              "  'ascertaining',\n",
              "  'the',\n",
              "  'dimensions',\n",
              "  'of',\n",
              "  'my',\n",
              "  'dungeon',\n",
              "  ';',\n",
              "  'as',\n",
              "  'I',\n",
              "  'might',\n",
              "  'make',\n",
              "  'its',\n",
              "  'circuit-',\n",
              "  'and',\n",
              "  'return',\n",
              "  'to',\n",
              "  'the',\n",
              "  'point',\n",
              "  'whence',\n",
              "  'I',\n",
              "  'set',\n",
              "  'out-',\n",
              "  'without',\n",
              "  'being',\n",
              "  'aware',\n",
              "  'of',\n",
              "  'the',\n",
              "  'fact',\n",
              "  ';',\n",
              "  'so',\n",
              "  'perfectly',\n",
              "  'uniform',\n",
              "  'seemed',\n",
              "  'the',\n",
              "  'wall',\n",
              "  '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing any punctuation\n",
        "punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~'\n",
        "\n",
        "from string import punctuation\n",
        "\n",
        "(punc_map := str.maketrans('', '', punctuation))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bRtDwQ8J_tU",
        "outputId": "c161e138-4392-4de9-ad00-f6bedacc26f9"
      },
      "id": "6bRtDwQ8J_tU",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{33: None,\n",
              " 34: None,\n",
              " 35: None,\n",
              " 36: None,\n",
              " 37: None,\n",
              " 38: None,\n",
              " 39: None,\n",
              " 40: None,\n",
              " 41: None,\n",
              " 42: None,\n",
              " 43: None,\n",
              " 44: None,\n",
              " 45: None,\n",
              " 46: None,\n",
              " 47: None,\n",
              " 58: None,\n",
              " 59: None,\n",
              " 60: None,\n",
              " 61: None,\n",
              " 62: None,\n",
              " 63: None,\n",
              " 64: None,\n",
              " 91: None,\n",
              " 92: None,\n",
              " 93: None,\n",
              " 94: None,\n",
              " 95: None,\n",
              " 96: None,\n",
              " 123: None,\n",
              " 124: None,\n",
              " 125: None,\n",
              " 126: None}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing punctuation from each word in the tokenized lists\n",
        "text_no_punc = [[word.translate(punc_map) for word in words] for words in text_words]\n",
        "text_no_punc[:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oi8UxuahJcLV",
        "outputId": "2f9a25db-9605-45e2-a0b3-7c03f2bb27f4"
      },
      "id": "oi8UxuahJcLV",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['This',\n",
              "  'process',\n",
              "  'however',\n",
              "  'afforded',\n",
              "  'me',\n",
              "  'no',\n",
              "  'means',\n",
              "  'of',\n",
              "  'ascertaining',\n",
              "  'the',\n",
              "  'dimensions',\n",
              "  'of',\n",
              "  'my',\n",
              "  'dungeon',\n",
              "  '',\n",
              "  'as',\n",
              "  'I',\n",
              "  'might',\n",
              "  'make',\n",
              "  'its',\n",
              "  'circuit',\n",
              "  'and',\n",
              "  'return',\n",
              "  'to',\n",
              "  'the',\n",
              "  'point',\n",
              "  'whence',\n",
              "  'I',\n",
              "  'set',\n",
              "  'out',\n",
              "  'without',\n",
              "  'being',\n",
              "  'aware',\n",
              "  'of',\n",
              "  'the',\n",
              "  'fact',\n",
              "  '',\n",
              "  'so',\n",
              "  'perfectly',\n",
              "  'uniform',\n",
              "  'seemed',\n",
              "  'the',\n",
              "  'wall',\n",
              "  '']]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Pipeable Solution:**"
      ],
      "metadata": {
        "id": "B17gJmG_gT4Z"
      },
      "id": "B17gJmG_gT4Z"
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeable function for readability\n",
        "(clean_words :=\n",
        " [record['text'] for record in records]\n",
        " >> pipeable(lambda texts: [s.lower() for s in texts])\n",
        " >> pipeable(lambda texts: [s.translate(punc_map) for s in texts])\n",
        " >> pipeable(lambda texts: [word_tokenize(s) for s in texts])\n",
        ")[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcXxavwqtDkb",
        "outputId": "6bf5a803-5c5e-4552-8ffc-327e6eb2d688"
      },
      "id": "CcXxavwqtDkb",
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['this',\n",
              "  'process',\n",
              "  'however',\n",
              "  'afforded',\n",
              "  'me',\n",
              "  'no',\n",
              "  'means',\n",
              "  'of',\n",
              "  'ascertaining',\n",
              "  'the',\n",
              "  'dimensions',\n",
              "  'of',\n",
              "  'my',\n",
              "  'dungeon',\n",
              "  'as',\n",
              "  'i',\n",
              "  'might',\n",
              "  'make',\n",
              "  'its',\n",
              "  'circuit',\n",
              "  'and',\n",
              "  'return',\n",
              "  'to',\n",
              "  'the',\n",
              "  'point',\n",
              "  'whence',\n",
              "  'i',\n",
              "  'set',\n",
              "  'out',\n",
              "  'without',\n",
              "  'being',\n",
              "  'aware',\n",
              "  'of',\n",
              "  'the',\n",
              "  'fact',\n",
              "  'so',\n",
              "  'perfectly',\n",
              "  'uniform',\n",
              "  'seemed',\n",
              "  'the',\n",
              "  'wall'],\n",
              " ['it',\n",
              "  'never',\n",
              "  'once',\n",
              "  'occurred',\n",
              "  'to',\n",
              "  'me',\n",
              "  'that',\n",
              "  'the',\n",
              "  'fumbling',\n",
              "  'might',\n",
              "  'be',\n",
              "  'a',\n",
              "  'mere',\n",
              "  'mistake']]"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 4: Map the text labels to numbers.**\n",
        "\n",
        "I need to combine clean_words with author in a single record before mapping numbers."
      ],
      "metadata": {
        "id": "vxrSHlc-2zaF"
      },
      "id": "vxrSHlc-2zaF"
    },
    {
      "cell_type": "code",
      "source": [
        "# Lables from kaggle website - only 3 authors\n",
        "labels = ['EAP', 'HPL', 'MWS']\n",
        "label_map = {'EAP': 0, 'HPL': 1, 'MWS': 2}\n",
        "\n",
        "# Map the labels to numbers\n",
        "numeric_labels = [label_map[label] for label in labels]\n",
        "numeric_labels"
      ],
      "metadata": {
        "id": "Njvoy1e023SF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88a7c536-e6d7-444e-f82a-f7ab65f025a3"
      },
      "id": "Njvoy1e023SF",
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data = [\n",
        "    {'text': text, 'label': label}\n",
        "    for text, label in zip(clean_words, numeric_labels)\n",
        "]\n",
        "\n",
        "cleaned_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoNb6kn8dk1E",
        "outputId": "e5afb778-f663-465c-df94-9e6e6e6e3b53"
      },
      "id": "hoNb6kn8dk1E",
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': ['this',\n",
              "   'process',\n",
              "   'however',\n",
              "   'afforded',\n",
              "   'me',\n",
              "   'no',\n",
              "   'means',\n",
              "   'of',\n",
              "   'ascertaining',\n",
              "   'the',\n",
              "   'dimensions',\n",
              "   'of',\n",
              "   'my',\n",
              "   'dungeon',\n",
              "   'as',\n",
              "   'i',\n",
              "   'might',\n",
              "   'make',\n",
              "   'its',\n",
              "   'circuit',\n",
              "   'and',\n",
              "   'return',\n",
              "   'to',\n",
              "   'the',\n",
              "   'point',\n",
              "   'whence',\n",
              "   'i',\n",
              "   'set',\n",
              "   'out',\n",
              "   'without',\n",
              "   'being',\n",
              "   'aware',\n",
              "   'of',\n",
              "   'the',\n",
              "   'fact',\n",
              "   'so',\n",
              "   'perfectly',\n",
              "   'uniform',\n",
              "   'seemed',\n",
              "   'the',\n",
              "   'wall'],\n",
              "  'label': 0},\n",
              " {'text': ['it',\n",
              "   'never',\n",
              "   'once',\n",
              "   'occurred',\n",
              "   'to',\n",
              "   'me',\n",
              "   'that',\n",
              "   'the',\n",
              "   'fumbling',\n",
              "   'might',\n",
              "   'be',\n",
              "   'a',\n",
              "   'mere',\n",
              "   'mistake'],\n",
              "  'label': 1},\n",
              " {'text': ['in',\n",
              "   'his',\n",
              "   'left',\n",
              "   'hand',\n",
              "   'was',\n",
              "   'a',\n",
              "   'gold',\n",
              "   'snuff',\n",
              "   'box',\n",
              "   'from',\n",
              "   'which',\n",
              "   'as',\n",
              "   'he',\n",
              "   'capered',\n",
              "   'down',\n",
              "   'the',\n",
              "   'hill',\n",
              "   'cutting',\n",
              "   'all',\n",
              "   'manner',\n",
              "   'of',\n",
              "   'fantastic',\n",
              "   'steps',\n",
              "   'he',\n",
              "   'took',\n",
              "   'snuff',\n",
              "   'incessantly',\n",
              "   'with',\n",
              "   'an',\n",
              "   'air',\n",
              "   'of',\n",
              "   'the',\n",
              "   'greatest',\n",
              "   'possible',\n",
              "   'self',\n",
              "   'satisfaction'],\n",
              "  'label': 2}]"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the label map for authors (mapping authors to numeric labels)\n",
        "author_map = {'EAP': 0, 'HPL': 1, 'MWS': 2}  # Extend the map as needed\n",
        "\n",
        "# Combine clean_words and author info into a record\n",
        "cleaned_data = [\n",
        "    {'text': text, 'author': record['author'], 'label': author_map.get(record['author'], -1)}\n",
        "    for text, record in zip(clean_words, records)\n",
        "]\n",
        "\n",
        "# Print the cleaned data with text, author, and numeric labels\n",
        "cleaned_data[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYI7w9b1kBbY",
        "outputId": "bee6c8af-d4e5-4693-f544-b19876eee82e"
      },
      "id": "QYI7w9b1kBbY",
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': ['this',\n",
              "   'process',\n",
              "   'however',\n",
              "   'afforded',\n",
              "   'me',\n",
              "   'no',\n",
              "   'means',\n",
              "   'of',\n",
              "   'ascertaining',\n",
              "   'the',\n",
              "   'dimensions',\n",
              "   'of',\n",
              "   'my',\n",
              "   'dungeon',\n",
              "   'as',\n",
              "   'i',\n",
              "   'might',\n",
              "   'make',\n",
              "   'its',\n",
              "   'circuit',\n",
              "   'and',\n",
              "   'return',\n",
              "   'to',\n",
              "   'the',\n",
              "   'point',\n",
              "   'whence',\n",
              "   'i',\n",
              "   'set',\n",
              "   'out',\n",
              "   'without',\n",
              "   'being',\n",
              "   'aware',\n",
              "   'of',\n",
              "   'the',\n",
              "   'fact',\n",
              "   'so',\n",
              "   'perfectly',\n",
              "   'uniform',\n",
              "   'seemed',\n",
              "   'the',\n",
              "   'wall'],\n",
              "  'author': 'EAP\"',\n",
              "  'label': -1},\n",
              " {'text': ['it',\n",
              "   'never',\n",
              "   'once',\n",
              "   'occurred',\n",
              "   'to',\n",
              "   'me',\n",
              "   'that',\n",
              "   'the',\n",
              "   'fumbling',\n",
              "   'might',\n",
              "   'be',\n",
              "   'a',\n",
              "   'mere',\n",
              "   'mistake'],\n",
              "  'author': 'HPL\"',\n",
              "  'label': -1},\n",
              " {'text': ['in',\n",
              "   'his',\n",
              "   'left',\n",
              "   'hand',\n",
              "   'was',\n",
              "   'a',\n",
              "   'gold',\n",
              "   'snuff',\n",
              "   'box',\n",
              "   'from',\n",
              "   'which',\n",
              "   'as',\n",
              "   'he',\n",
              "   'capered',\n",
              "   'down',\n",
              "   'the',\n",
              "   'hill',\n",
              "   'cutting',\n",
              "   'all',\n",
              "   'manner',\n",
              "   'of',\n",
              "   'fantastic',\n",
              "   'steps',\n",
              "   'he',\n",
              "   'took',\n",
              "   'snuff',\n",
              "   'incessantly',\n",
              "   'with',\n",
              "   'an',\n",
              "   'air',\n",
              "   'of',\n",
              "   'the',\n",
              "   'greatest',\n",
              "   'possible',\n",
              "   'self',\n",
              "   'satisfaction'],\n",
              "  'author': 'EAP\"',\n",
              "  'label': -1}]"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PHzM9KlY0fDa",
        "KZqtM8PqyPwC",
        "S5E1KxPygK2i"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}